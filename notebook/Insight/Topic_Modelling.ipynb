{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yk5iXLnPmT4u"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from gensim import corpora\n",
        "import nltk\n",
        "\n",
        "# Membaca Data dari CSV\n",
        "df = pd.read_csv('youtube_comments_technology.csv')\n",
        "\n",
        "# Preprocessing untuk Topic Modeling\n",
        "def preprocess(text):\n",
        "    return [word for word in text.lower().split() if word.isalpha()]\n",
        "\n",
        "df['processed_comments'] = df['textDisplay'].apply(preprocess)\n",
        "\n",
        "# Membuat Dictionary dan Corpus untuk LDA\n",
        "dictionary = corpora.Dictionary(df['processed_comments'])\n",
        "corpus = [dictionary.doc2bow(text) for text in df['processed_comments']]\n",
        "\n",
        "# Latent Dirichlet Allocation (LDA)\n",
        "lda_model = LatentDirichletAllocation(n_components=5, random_state=42)\n",
        "lda_model.fit(corpus)\n",
        "\n",
        "# Menampilkan Topik\n",
        "for idx, topic in enumerate(lda_model.components_):\n",
        "    print(f\"Topic {idx}:\")\n",
        "    print([dictionary[i] for i in topic.argsort()[-10:]])"
      ]
    }
  ]
}